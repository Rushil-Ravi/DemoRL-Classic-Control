# ðŸŽ® From Imitation to Optimization: Evaluating Demonstrators in Classic Control Environments

**Course:** Deep Reinforcement Learning  
**Team:** Rushil Ravi (UIN: 836000314) & Isabel Moore (UIN: 229001058)  

##  Project Overview

This project investigates whether starting from expert demonstrations accelerates reinforcement learning training in classic control environments. We implement a complete pipeline: Expert DQN â†’ Behavior Cloning â†’ PPO fine-tuning, comparing Pure RL vs BCâ†’RL approaches.

##  Key Findings

âœ… **79.6% faster learning** with BC initialization on CartPole-v1  
âœ… **Perfect imitation** by Behavior Cloning (500.00 Â± 0.00 reward)  
âœ… **Statistical significance** proven through extensive evaluation  
âœ… **Sample efficiency** dramatically improved with demonstrations

##  Quick Start

### 1. Installation
```bash
# Clone repository
git clone https://github.com/yourusername/DemoRL-Classic-Control.git
cd DemoRL-Classic-Control

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Complete Pipeline
```bash
# For CartPole-v1 (recommended)
python main.py --env CartPole-v1 --mode all

# Run individual steps
python main.py --env CartPole-v1 --mode expert    # Train expert
python main.py --env CartPole-v1 --mode demos     # Collect demonstrations
python main.py --env CartPole-v1 --mode bc        # Train behavior cloning
python main.py --env CartPole-v1 --mode rl        # Compare RL methods
python main.py --env CartPole-v1 --mode eval      # Final evaluation

```

### 3. Results Summary

### CartPole-v1 Results:

| Method  | Average Reward    | Episodes to Threshold | Improvement       |
|---------|-------------------|-----------------------|-------------------|
| Pure RL | 44.06 Â± 30.91     | 270 episodes          | Baseline          |
| BC-only | **500.00 Â± 0.00** | 1 episode             | Perfect imitation |
| BCâ†’RL   | 76.22 Â± 54.49     | **55 episodes**       | **79.6% faster**  |


### Key Findings:

1. BC perfectly imitates expert achieving maximum reward (500)

2. BCâ†’RL learns 79.6% faster than Pure RL

3. Demonstrations provide strong priors for RL training

4. Sample efficiency is dramatically improved


### 4. Project Structure

```

DemoRL-Classic-Control/
â”œâ”€â”€ main.py                # Main runner script
â”œâ”€â”€ scripts/               # Training and evaluation scripts
â”‚   â”œâ”€â”€ train_expert.py    # DQN expert training
â”‚   â”œâ”€â”€ collect_demos.py   # Demonstration collection
â”‚   â”œâ”€â”€ train_bc.py        # Behavior cloning
â”‚   â”œâ”€â”€ train_rl.py        # RL comparison (Pure RL vs BCâ†’RL)
â”‚   â””â”€â”€ evaluate.py        # Final evaluation
â”œâ”€â”€ src/                   # Core modules
â”‚   â”œâ”€â”€ environments.py    # Environment wrapper
â”‚   â”œâ”€â”€ networks.py        # Neural network architectures
â”‚   â”œâ”€â”€ dqn_agent.py       # DQN implementation
â”‚   â”œâ”€â”€ bc_agent.py        # Behavior cloning agent
â”‚   â”œâ”€â”€ ppo_agent.py       # PPO agent
â”‚   â””â”€â”€ utils.py           # Utilities (replay buffer, etc.)
â”œâ”€â”€ results/               # Configuration files
â”‚	â”œâ”€â”€ models.py    	   # models
â”‚   â”œâ”€â”€ plots.py           # evaluation plots
â”‚	â”œâ”€â”€ summary.md         # Short summary of results  
â””â”€â”€ README.md              # This file
```


### 5. Generated Outputs

After running the pipeline, you'll get:

### Models:
1. expert_CartPole-v1.pth - Expert DQN model

2. demos_CartPole-v1.pkl - Expert demonstrations

3. bc_CartPole-v1.pth - Behavior cloning model

4. pure_rl_CartPole-v1.pth - Pure PPO model

5. bc_rl_CartPole-v1.pth - BC-initialized PPO model

### Plots:
1. expert_training_CartPole-v1.png - Expert learning curve

2. bc_training_CartPole-v1.png - BC training loss

3. comparison_CartPole-v1.png - Pure RL vs BCâ†’RL comparison


###6. Technical Implementation

Algorithms Implemented:
1. Deep Q-Network (DQN) for expert training

2. Behavior Cloning (BC) for imitation learning

3. Proximal Policy Optimization (PPO) for RL fine-tuning

4. Experience Replay for stable training

5. Epsilon-greedy exploration strategy


###7. Team Contributions

Rushil Ravi:

- Expert DQN implementation

- Demonstration collection pipeline

- Project infrastructure and testing

- Code documentation


Isabel Moore:

- Experimental design and methodology

- Statistical analysis and evaluation

- Literature Survey

- Final report


###8. Acknowledgments

- OpenAI Gym for the environments

- PyTorch team for the deep learning framework

- Course instructors for guidance and feedback

- Reinforcement learning research community

